{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "dparser = lambda x: datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ')\n",
    "df = pd.read_csv(\n",
    "    'in_lang_cleaned_extended.tsv',\n",
    "    header=None,\n",
    "    names = ['uuid', 'lang', 'aid', 'published', 'category', 'journal_ref', 'bibitem_str'],\n",
    "    sep='\\t',\n",
    "    parse_dates=['published'],\n",
    "    date_parser=dparser,\n",
    "    low_memory=False,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1992: {'cs': 0, 'math': 0, 'other': 0, 'phys': 18},\n",
       " 1993: {'cs': 0, 'math': 2, 'other': 0, 'phys': 62},\n",
       " 1994: {'cs': 1, 'math': 8, 'other': 0, 'phys': 74},\n",
       " 1995: {'cs': 1, 'math': 24, 'other': 1, 'phys': 113},\n",
       " 1996: {'cs': 3, 'math': 22, 'other': 5, 'phys': 135},\n",
       " 1997: {'cs': 1, 'math': 18, 'other': 5, 'phys': 189},\n",
       " 1998: {'cs': 2, 'math': 33, 'other': 0, 'phys': 260},\n",
       " 1999: {'cs': 4, 'math': 46, 'other': 0, 'phys': 297},\n",
       " 2000: {'cs': 7, 'math': 62, 'other': 0, 'phys': 378},\n",
       " 2001: {'cs': 12, 'math': 70, 'other': 0, 'phys': 318},\n",
       " 2002: {'cs': 6, 'math': 86, 'other': 0, 'phys': 359},\n",
       " 2003: {'cs': 7, 'math': 101, 'other': 1, 'phys': 381},\n",
       " 2004: {'cs': 9, 'math': 141, 'other': 2, 'phys': 387},\n",
       " 2005: {'cs': 10, 'math': 169, 'other': 2, 'phys': 410},\n",
       " 2006: {'cs': 24, 'math': 217, 'other': 4, 'phys': 435},\n",
       " 2007: {'cs': 28, 'math': 274, 'other': 6, 'phys': 407},\n",
       " 2008: {'cs': 37, 'math': 315, 'other': 15, 'phys': 363},\n",
       " 2009: {'cs': 39, 'math': 383, 'other': 17, 'phys': 456},\n",
       " 2010: {'cs': 57, 'math': 467, 'other': 15, 'phys': 435},\n",
       " 2011: {'cs': 58, 'math': 518, 'other': 9, 'phys': 438},\n",
       " 2012: {'cs': 88, 'math': 550, 'other': 17, 'phys': 444},\n",
       " 2013: {'cs': 94, 'math': 659, 'other': 26, 'phys': 464},\n",
       " 2014: {'cs': 105, 'math': 698, 'other': 24, 'phys': 447},\n",
       " 2015: {'cs': 96, 'math': 710, 'other': 24, 'phys': 470},\n",
       " 2016: {'cs': 87, 'math': 755, 'other': 22, 'phys': 398},\n",
       " 2017: {'cs': 118, 'math': 750, 'other': 33, 'phys': 435},\n",
       " 2018: {'cs': 116, 'math': 758, 'other': 44, 'phys': 384},\n",
       " 2019: {'cs': 9, 'math': 52, 'other': 3, 'phys': 32}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Absolute number of papers with E-to-nE references per year per discipline\n",
    "\n",
    "disc_map = {  # see: https://arxiv.org/category_taxonomy\n",
    "    'math': 'math',\n",
    "    'cond-mat': 'phys',\n",
    "    'math-ph': 'phys',\n",
    "    'physics': 'phys',\n",
    "    'cs': 'cs',\n",
    "    'hep-th': 'phys',\n",
    "    'nlin': 'phys',\n",
    "    'gr-qc': 'phys',\n",
    "    'hep-ph': 'phys',\n",
    "    'astro-ph': 'phys',\n",
    "    'quant-ph': 'phys',\n",
    "    'nucl-th': 'phys',\n",
    "    'q-fin': 'other',\n",
    "    'hep-ex': 'phys',\n",
    "    'stat': 'other',\n",
    "    'nucl-ex': 'phys',\n",
    "    'q-alg': 'math',\n",
    "    'q-bio': 'other',\n",
    "    'solv-int': 'phys',\n",
    "    'chao-dyn': 'phys',\n",
    "    'hep-lat': 'phys',\n",
    "    'funct-an': 'math',\n",
    "    'patt-sol': 'phys',\n",
    "    'dg-ga': 'other',\n",
    "    'alg-geom': 'math',\n",
    "    'cmp-lg': 'cs',\n",
    "    'adap-org': 'phys',\n",
    "    'econ': 'other',\n",
    "    'eess': 'other',\n",
    "    'acc-phys': 'phys',\n",
    "    'comp-gas': 'cs'\n",
    "}\n",
    "\n",
    "df.groupby(\n",
    "    [\n",
    "        df['category'].apply(lambda x: disc_map[x.split('.')[0]]),\n",
    "        df['published'].dt.year\n",
    "    ]\n",
    ").agg({'aid': pd.Series.nunique}).unstack(fill_value=0)['aid'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192097"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load unarXive ID set (arXiv IDs w/o slashes)\n",
    "\n",
    "unarXive_citing_aids = []\n",
    "with open('unarXive_unique_aids') as f:\n",
    "    for line in f:\n",
    "        unarXive_citing_aids.append(line.strip())\n",
    "len(unarXive_citing_aids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load arXiv metadata (from arXiv metadata dump; arXiv IDs w/ slashes)\n",
    "\n",
    "fos_map = {\n",
    "    'physics': 'phys',\n",
    "    'math': 'math',\n",
    "    'cs': 'cs',\n",
    "    'eess': 'other',\n",
    "    'econ': 'other'\n",
    "}\n",
    "\n",
    "dparser2 = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "df_ameta = pd.read_csv(\n",
    "    'aid_to_cat_n_date.csv',\n",
    "    sep=',',\n",
    "    parse_dates=['date'],\n",
    "    date_parser=dparser2,\n",
    "    low_memory=False,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "df_ameta.fos = df_ameta.fos.apply(lambda x: fos_map[x])  # align disciplines\n",
    "df_ameta.aid = df_ameta.aid.apply(lambda x: x.replace('/', ''))  # align IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<year>: <control> -> <calculated> (<error>)\n",
      "1991: 306 -> 370 (20.92%)\n",
      "1992: 3263 -> 3181 (2.51%)\n",
      "1993: 6743 -> 6524 (3.25%)\n",
      "1994: 10097 -> 9893 (2.02%)\n",
      "1995: 13014 -> 12748 (2.04%)\n",
      "1996: 15866 -> 15578 (1.82%)\n",
      "1997: 19624 -> 19231 (2.00%)\n",
      "1998: 24172 -> 23684 (2.02%)\n",
      "1999: 27704 -> 27174 (1.91%)\n",
      "2000: 30601 -> 30245 (1.16%)\n",
      "2001: 33214 -> 32719 (1.49%)\n",
      "2002: 36121 -> 35664 (1.27%)\n",
      "2003: 39414 -> 38849 (1.43%)\n",
      "2004: 43727 -> 42914 (1.86%)\n",
      "2005: 46855 -> 46021 (1.78%)\n",
      "2006: 50227 -> 49377 (1.69%)\n",
      "2007: 55638 -> 54501 (2.04%)\n",
      "2008: 58915 -> 57404 (2.56%)\n",
      "2009: 64047 -> 62663 (2.16%)\n",
      "2010: 70131 -> 68951 (1.68%)\n",
      "2011: 76578 -> 75120 (1.90%)\n",
      "2012: 84603 -> 82561 (2.41%)\n",
      "2013: 92641 -> 90320 (2.51%)\n",
      "2014: 97517 -> 94798 (2.79%)\n",
      "2015: 105280 -> 101971 (3.14%)\n",
      "2016: 113380 -> 109946 (3.03%)\n",
      "2017: 123523 -> 119812 (3.00%)\n",
      "2018: 140616 -> 136680 (2.80%)\n",
      "2019: 11537 -> 152210 (1219.32%)\n",
      "2020: 67286 -> 5127 (92.38%)\n"
     ]
    }
   ],
   "source": [
    "# Check validity against monthly sumbmission numbers from arXiv website\n",
    "#\n",
    "# (slight differences possible b/c metadata dump dating works a bit different;\n",
    "#  e.g. also contains dates prior to 1991)\n",
    "\n",
    "arxiv_monthly_df = pd.read_csv('arXiv_monthly_submissions.csv')\n",
    "total_submissions_per_year = {}\n",
    "for i, row in arxiv_monthly_df.iterrows():\n",
    "    year, month = row.month.split('-')\n",
    "    if int(year) == df['published'].max().year and int(month) > df['published'].max().month:\n",
    "        continue\n",
    "    if year not in total_submissions_per_year:\n",
    "        total_submissions_per_year[year] = 0\n",
    "    total_submissions_per_year[year] += row.submissions\n",
    "calc_subs_per_year = df_ameta.date.dt.year.value_counts().to_dict()\n",
    "print('<year>: <control> -> <calculated> (<error>)')\n",
    "for year, control_count in total_submissions_per_year.items():\n",
    "    print('{}: {} -> {} ({:.2f}%)'.format(\n",
    "        year,\n",
    "        control_count,\n",
    "        calc_subs_per_year[int(year)],\n",
    "        (abs(control_count-calc_subs_per_year[int(year)])/control_count)*100\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1990: {'cs': 0, 'math': 7, 'other': 0, 'phys': 0},\n",
       " 1991: {'cs': 0, 'math': 15, 'other': 0, 'phys': 84},\n",
       " 1992: {'cs': 0, 'math': 40, 'other': 0, 'phys': 1274},\n",
       " 1993: {'cs': 0, 'math': 86, 'other': 0, 'phys': 3218},\n",
       " 1994: {'cs': 39, 'math': 172, 'other': 0, 'phys': 5280},\n",
       " 1995: {'cs': 43, 'math': 369, 'other': 0, 'phys': 7241},\n",
       " 1996: {'cs': 59, 'math': 454, 'other': 0, 'phys': 10652},\n",
       " 1997: {'cs': 80, 'math': 647, 'other': 0, 'phys': 13885},\n",
       " 1998: {'cs': 129, 'math': 945, 'other': 0, 'phys': 17143},\n",
       " 1999: {'cs': 188, 'math': 1228, 'other': 0, 'phys': 19646},\n",
       " 2000: {'cs': 283, 'math': 1589, 'other': 0, 'phys': 21956},\n",
       " 2001: {'cs': 271, 'math': 1763, 'other': 0, 'phys': 24069},\n",
       " 2002: {'cs': 351, 'math': 2302, 'other': 0, 'phys': 26231},\n",
       " 2003: {'cs': 431, 'math': 3180, 'other': 0, 'phys': 27688},\n",
       " 2004: {'cs': 530, 'math': 4334, 'other': 0, 'phys': 30115},\n",
       " 2005: {'cs': 716, 'math': 4947, 'other': 0, 'phys': 31727},\n",
       " 2006: {'cs': 880, 'math': 6510, 'other': 0, 'phys': 32748},\n",
       " 2007: {'cs': 1185, 'math': 8449, 'other': 1, 'phys': 34478},\n",
       " 2008: {'cs': 1424, 'math': 10042, 'other': 0, 'phys': 35119},\n",
       " 2009: {'cs': 2656, 'math': 11151, 'other': 5, 'phys': 37089},\n",
       " 2010: {'cs': 3134, 'math': 13027, 'other': 2, 'phys': 39134},\n",
       " 2011: {'cs': 3692, 'math': 15292, 'other': 2, 'phys': 42102},\n",
       " 2012: {'cs': 5487, 'math': 17562, 'other': 4, 'phys': 42612},\n",
       " 2013: {'cs': 7687, 'math': 19301, 'other': 9, 'phys': 45315},\n",
       " 2014: {'cs': 9267, 'math': 21102, 'other': 6, 'phys': 46025},\n",
       " 2015: {'cs': 11667, 'math': 22876, 'other': 9, 'phys': 48443},\n",
       " 2016: {'cs': 15817, 'math': 24222, 'other': 33, 'phys': 49397},\n",
       " 2017: {'cs': 21229, 'math': 25274, 'other': 436, 'phys': 50469},\n",
       " 2018: {'cs': 29707, 'math': 26540, 'other': 1732, 'phys': 52635},\n",
       " 2019: {'cs': 2445, 'math': 2206, 'other': 119, 'phys': 4177}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For some reason the arXiv metadata dump doesn't contain any\n",
    "# non-phys/math/cs records until 2004\n",
    "\n",
    "df_ameta.groupby(\n",
    "    [\n",
    "        df_ameta.fos,\n",
    "        df_ameta.date.dt.year\n",
    "    ]\n",
    ").agg({'aid': pd.Series.count}).unstack(fill_value=0)['aid'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sampling candidates to arXiv IDs included in unarXive\n",
    "\n",
    "df_sampling_candidates = df_ameta[df_ameta.aid.isin(unarXive_citing_aids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 18171\n",
      "Control size: 18171\n"
     ]
    }
   ],
   "source": [
    "# Generate random sample\n",
    "\n",
    "from random import sample\n",
    "\n",
    "year_disc_distrib = df.groupby(\n",
    "    [\n",
    "        df['category'].apply(lambda x: disc_map[x.split('.')[0]]),\n",
    "        df['published'].dt.year\n",
    "    ]\n",
    ").agg({'aid': pd.Series.nunique}).unstack(fill_value=0)['aid'].to_dict()\n",
    "\n",
    "sample_dict = {}\n",
    "sample_list = []\n",
    "other_compensation = 0\n",
    "total_count = 0\n",
    "for year, distrib_dict in year_disc_distrib.items():\n",
    "    # iterate through years\n",
    "    sample_dict[year] = {}\n",
    "    for disc, count in distrib_dict.items():\n",
    "        # iterate through disciplines\n",
    "        total_count += count\n",
    "        candidate_list = df_sampling_candidates[\n",
    "            (df_sampling_candidates.date.dt.year == year) & (df_sampling_candidates.fos == disc)\n",
    "        ]['aid'].to_list()\n",
    "        if disc == 'other':\n",
    "            ocount = count + other_compensation\n",
    "            if ocount > len(candidate_list):\n",
    "                # take compensation from following year\n",
    "                smpl = candidate_list\n",
    "                other_compensation += count - len(candidate_list)\n",
    "            else:\n",
    "                smpl = sample(candidate_list, ocount)\n",
    "                other_compensation = 0\n",
    "        else:\n",
    "            smpl = sample(candidate_list, count)\n",
    "        sample_dict[year][disc] = smpl\n",
    "        sample_list.extend(smpl)\n",
    "print('Sample size: {}'.format(len(sample_list)))\n",
    "print('Control size: {}'.format(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "\n",
    "with open('control_sample_18171', 'w') as f:\n",
    "    for aid in sample_list:\n",
    "        f.write(f'{aid}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
